{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training and testing the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "import gzip\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from typing import Tuple"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You've already seen how to train a neural network using Keras in [module 24](/module24/lecture/train.ipynb) &mdash; in this notebook, we'll re-implement the training loop in TensorFlow. This will help you understand what goes on under the hood a bit better, will give you the opportunity to customize the training loop if you want, and will enable you to debug it.\n",
        "\n",
        "We'll start by including code that gives us the datasets and model that we'll use in the remainder of this notebook. We will use the same FashionMNIST dataset and data loading code as in [module 24](/module24/lecture/train.ipynb), so feel free to re-visit that module if something is not clear, or take a look [at the source code](https://github.com/MicrosoftDocs/tensorflow-learning-path/blob/main/intro-tf/tintro.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wget\n",
        "\n",
        "if not os.path.exists('tintro.py'):\n",
        "    wget.download('https://raw.githubusercontent.com/MicrosoftDocs/tensorflow-learning-path/main/intro-tf/tintro.py', 'tintro.py') \n",
        "\n",
        "from tintro import *"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we mentioned in module 1, the goal of training the neural network is to find parameters $W$ and $b$ that minimize the loss function, which measures the difference between the actual and predicted labels. We also mentioned that we can think of the neural network as the function $\\ell$ below, and that we use an optimization algorithm to find the parameters $W$ and $b$ that minimize this function.\n",
        "\n",
        "$$\n",
        "\\mathrm{loss} = \\ell(X, y, W, b)\n",
        "$$\n",
        "\n",
        "Let's now dig deeper into what this optimization algorithm might look like. There are many types of optimization algorithms, but in this tutorial we'll cover only the simplest one: the gradient descent algorithm. To implement gradient descent, we iteratively improve our estimates of $W$ and $b$ according to the update formulas below, until the gradients are smaller than a pre-defined threshold $\\epsilon$ (or for a pre-defined number of times):\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "  W &:= W - \\alpha \\frac{\\partial \\ell}{\\partial W} \\\\\n",
        "  b &:= b - \\alpha \\frac{\\partial \\ell}{\\partial b}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "The parameter $\\alpha$ is typically referred to as the \"learning rate,\" and will be defined later in the code. \n",
        "\n",
        "When doing training, we pass a mini-batch of data as input, perform a sequence of calculations to obtain the loss, then propagate back through the network to calculate the derivatives used in the gradient descent formulas above. Once we have the derivatives, we can update the values of the network's parameters $W$ and $b$ according to the formulas. This sequence of steps is the backpropagation algorithm. By performing these calculations several times, our parameters get updated repeatedly, getting more and more accurate each time. \n",
        "\n",
        "In Keras, when we called the function [`fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit), the backpropagation algorithm was executed several times. Here, we'll start by understanding the code that reflects a single pass of the backpropagation algorithm:\n",
        "\n",
        "- A forward pass through the model to compute the predicted value, `y_prime = model(X, training=True)`\n",
        "- A calculation of the loss using a loss function, `loss = loss_fn(y, y_prime)`\n",
        "- A backward pass from the loss function through the model to calculate derivatives, `grads = tape.gradient(loss, model.trainable_variables)`\n",
        "- A gradient descent step to update $W$ and $b$ using the derivatives calculated in the backward pass, `optimizer.apply_gradients(zip(grads, model.trainable_variables))`\n",
        "\n",
        "Here's the complete code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_one_batch(X, y, model, loss_fn, optimizer) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_prime = model(X, training=True)\n",
        "    loss = loss_fn(y, y_prime)\n",
        "\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return (y_prime, loss)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that the code above ensures that the forward calculations are within the `GradientTape`'s scope, just as we saw in the previous notebook. This makes it possible for us to ask the tape for the gradients. \n",
        "\n",
        "The code above works for a single mini-batch, which is typically much smaller than the full set of data (in this sample we use a mini-batch of size 64, out of 60,000 training data items). But we want to execute the backpropagation algorithm for the full set of data. We can do so by iterating through the `Dataset` we created earlier, which, as we saw in module 1, returns a mini-batch per iteration. There are two critical lines in the code below: the `for` loop and the call to the `fit_one_batch` function. The rest of the code just prints the accuracy and loss as the model is being trained. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit(dataset: tf.data.Dataset, model: tf.keras.Model, loss_fn: tf.keras.losses.Loss, \n",
        "optimizer: tf.optimizers.Optimizer) -> None:\n",
        "  batch_count = len(dataset)\n",
        "  loss_sum = 0\n",
        "  correct_item_count = 0\n",
        "  current_item_count = 0\n",
        "  print_every = 100\n",
        "\n",
        "  for batch_index, (X, y) in enumerate(dataset):\n",
        "    (y_prime, loss) = fit_one_batch(X, y, model, loss_fn, optimizer)\n",
        "\n",
        "    y = tf.cast(y, tf.int64)\n",
        "    correct_item_count += (tf.math.argmax(y_prime, axis=1) == y).numpy().sum()\n",
        "\n",
        "    batch_loss = loss.numpy()\n",
        "    loss_sum += batch_loss\n",
        "    current_item_count += len(X)\n",
        "\n",
        "    if ((batch_index + 1) % print_every == 0) or ((batch_index + 1) == batch_count):\n",
        "      batch_accuracy = correct_item_count / current_item_count * 100\n",
        "      print(f'[Batch {batch_index + 1:>3d} - {current_item_count:>5d} items] accuracy: {batch_accuracy:>0.1f}%, loss: {batch_loss:>7f}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A complete iteration over all mini-batches in the dataset is called an \"epoch.\" In this sample, we restrict the code to just five epochs for quick execution, but in a real project you would want to set it to a much higher number (to achieve better predictions). The code below also shows the creation of the loss function and optimizer, which we discussed in module 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Daniel Vong\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Fitting:\n",
            "\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "[Batch 100 -  6400 items] accuracy: 60.2%, loss: 0.864982\n",
            "[Batch 200 - 12800 items] accuracy: 66.7%, loss: 0.611843\n",
            "[Batch 300 - 19200 items] accuracy: 70.3%, loss: 0.590243\n",
            "[Batch 400 - 25600 items] accuracy: 72.4%, loss: 0.725874\n",
            "[Batch 500 - 32000 items] accuracy: 74.1%, loss: 0.492054\n",
            "[Batch 600 - 38400 items] accuracy: 75.2%, loss: 0.538996\n",
            "[Batch 700 - 44800 items] accuracy: 76.1%, loss: 0.396010\n",
            "[Batch 800 - 51200 items] accuracy: 76.8%, loss: 0.518657\n",
            "[Batch 900 - 57600 items] accuracy: 77.3%, loss: 0.575013\n",
            "[Batch 938 - 60000 items] accuracy: 77.5%, loss: 0.433798\n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "[Batch 100 -  6400 items] accuracy: 83.0%, loss: 0.322842\n",
            "[Batch 200 - 12800 items] accuracy: 83.0%, loss: 0.511460\n",
            "[Batch 300 - 19200 items] accuracy: 82.8%, loss: 0.320734\n",
            "[Batch 400 - 25600 items] accuracy: 83.0%, loss: 0.387147\n",
            "[Batch 500 - 31968 items] accuracy: 82.9%, loss: 0.395419\n",
            "[Batch 600 - 38368 items] accuracy: 83.1%, loss: 0.507462\n",
            "[Batch 700 - 44768 items] accuracy: 83.2%, loss: 0.523528\n",
            "[Batch 800 - 51168 items] accuracy: 83.4%, loss: 0.486614\n",
            "[Batch 900 - 57568 items] accuracy: 83.4%, loss: 0.248777\n",
            "[Batch 938 - 60000 items] accuracy: 83.4%, loss: 0.492058\n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "[Batch 100 -  6400 items] accuracy: 84.6%, loss: 0.392557\n",
            "[Batch 200 - 12800 items] accuracy: 84.6%, loss: 0.492014\n",
            "[Batch 300 - 19200 items] accuracy: 84.7%, loss: 0.516763\n",
            "[Batch 400 - 25600 items] accuracy: 84.8%, loss: 0.388460\n",
            "[Batch 500 - 32000 items] accuracy: 84.7%, loss: 0.621309\n",
            "[Batch 600 - 38400 items] accuracy: 84.6%, loss: 0.496606\n",
            "[Batch 700 - 44768 items] accuracy: 84.6%, loss: 0.245792\n",
            "[Batch 800 - 51168 items] accuracy: 84.6%, loss: 0.431332\n",
            "[Batch 900 - 57568 items] accuracy: 84.6%, loss: 0.599762\n",
            "[Batch 938 - 60000 items] accuracy: 84.6%, loss: 0.287105\n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "[Batch 100 -  6400 items] accuracy: 85.2%, loss: 0.556293\n",
            "[Batch 200 - 12800 items] accuracy: 85.1%, loss: 0.326249\n",
            "[Batch 300 - 19200 items] accuracy: 85.2%, loss: 0.555748\n",
            "[Batch 400 - 25600 items] accuracy: 85.2%, loss: 0.276587\n",
            "[Batch 500 - 32000 items] accuracy: 85.2%, loss: 0.522097\n",
            "[Batch 600 - 38400 items] accuracy: 85.1%, loss: 0.341500\n",
            "[Batch 700 - 44768 items] accuracy: 85.1%, loss: 0.587119\n",
            "[Batch 800 - 51168 items] accuracy: 85.2%, loss: 0.462226\n",
            "[Batch 900 - 57568 items] accuracy: 85.2%, loss: 0.362722\n",
            "[Batch 938 - 60000 items] accuracy: 85.2%, loss: 0.475590\n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "[Batch 100 -  6400 items] accuracy: 86.3%, loss: 0.457503\n",
            "[Batch 200 - 12800 items] accuracy: 85.7%, loss: 0.306026\n",
            "[Batch 300 - 19200 items] accuracy: 85.8%, loss: 0.461718\n",
            "[Batch 400 - 25600 items] accuracy: 85.9%, loss: 0.526879\n",
            "[Batch 500 - 32000 items] accuracy: 85.9%, loss: 0.487633\n",
            "[Batch 600 - 38400 items] accuracy: 86.0%, loss: 0.354403\n",
            "[Batch 700 - 44800 items] accuracy: 86.0%, loss: 0.406079\n",
            "[Batch 800 - 51168 items] accuracy: 85.9%, loss: 0.525396\n",
            "[Batch 900 - 57568 items] accuracy: 85.9%, loss: 0.325505\n",
            "[Batch 938 - 60000 items] accuracy: 85.9%, loss: 0.414105\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.1\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "\n",
        "(train_dataset, test_dataset) = get_data(batch_size)\n",
        "\n",
        "model = NeuralNetwork()\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.optimizers.SGD(learning_rate)\n",
        "\n",
        "print('\\nFitting:')\n",
        "for epoch in range(epochs):\n",
        "  print(f'\\nEpoch {epoch + 1}\\n-------------------------------')\n",
        "  fit(train_dataset, model, loss_fn, optimizer)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "azureml_py38_PT_and_TF"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
